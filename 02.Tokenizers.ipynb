{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация\n",
    "\n",
    "Задача - разделить предложение на слова или отдельные элементы (знаки препинания, гиперссылки и т.д.), по возможности сохраняя какие-то атрибуты текста."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регурярные выражения\n",
    "\n",
    "В модуле `re` есть недокументированный класс `Scanner`, с помощью которого можно реализовать лексический анализатор. `Scanner` будет искать вхождения паттернов в тексте и на каждое совпадение вызывать соответствующую функцию. В общем случае подобный код неэффективен, лексические анализаторы лучше реализовывать с помощью специальных инструментов - генераторов лексических анализаторов, которые обеспечат анализ за линейное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Hello', 'word'),\n",
       "  (' ', 'whitespace'),\n",
       "  ('world', 'word'),\n",
       "  ('!', 'preposition')],\n",
       " '')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "scanner = re.Scanner(\n",
    "   [(r'(\\w+)@(\\w+)\\.(\\w{2,3})', lambda s, x: (x, 'email')),\n",
    "    (r'[a-zA-Z]+', lambda s, x: (x, 'word')), \n",
    "    (r'\\d+', lambda s, x: (x, 'digit')),    \n",
    "    (r'\\s+', lambda s, x: (x, 'whitespace')),\n",
    "    (r'[.,;\"!?:]', lambda s, x: (x, 'preposition')),\n",
    "    ])\n",
    "\n",
    "##scanner.scan('hello, world 1234 test@example.com')\n",
    "scanner.scan(\"Hello world!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "Natural Language Toolkit, библиотека для обработки естественных языков. Она создавалась для учебных целей, но тем не менее приобрела определенную популярность. Реализовано некоторое количество методов токенизации, которые можно использовать для повседневных задач и экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alex/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Hello world 4.2.\n",
      "word_tokenize:  ['Hello', 'world', '4.2', '.']\n",
      "wordpunct_tokenize:  ['Hello', 'world', '4', '.', '2', '.']\n",
      "tweet:  ['Hello', 'world', '4.2', '.']\n",
      "\n",
      "Sentence: LA New-York\n",
      "word_tokenize:  ['LA', 'New-York']\n",
      "wordpunct_tokenize:  ['LA', 'New', '-', 'York']\n",
      "tweet:  ['LA', 'New-York']\n",
      "\n",
      "Sentence: Hello world 4.2!\n",
      "word_tokenize:  ['Hello', 'world', '4.2', '!']\n",
      "wordpunct_tokenize:  ['Hello', 'world', '4', '.', '2', '!']\n",
      "tweet:  ['Hello', 'world', '4.2', '!']\n",
      "\n",
      "Sentence: Say me #hello\n",
      "word_tokenize:  ['Say', 'me', '#', 'hello']\n",
      "wordpunct_tokenize:  ['Say', 'me', '#', 'hello']\n",
      "tweet:  ['Say', 'me', '#hello']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize, TweetTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "\n",
    "sentences = [\"Hello world 4.2.\", \"LA New-York\", \"Hello world 4.2!\", \"Say me #hello\"]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(\"Sentence: {}\".format(sent))\n",
    "    print(\"word_tokenize: \", word_tokenize(sent))\n",
    "    print(\"wordpunct_tokenize: \", wordpunct_tokenize(sent)),\n",
    "    print(\"tweet: \", tweet_tokenize.tokenize(sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like trains.', 'I like cakes.', 'Dr. House, how are you?', 'I like I.B.M.', '!']\n",
      "[\"The world's oldest football competition is the FA Cup, which was founded by C. W. Alcock and has been contested by English teams since 1872.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(\n",
    "    sent_tokenize('I like trains. I like cakes. Dr. House, how are you? I like I.B.M.!'))\n",
    "print(\n",
    "    sent_tokenize('The world\\'s oldest football competition is the FA Cup, which was founded by C. W. Alcock and has been contested by English teams since 1872.'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ply\n",
    "\n",
    "Приведем лексического анализатора на `ply`. В данном случае анализатор описывается в классе, могут быть три вида токенов - слова, цифры и пробелы. Для каждого токена в тексте выозвращается необходимая информация - типа, длина смещение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,123,1,0)\n",
      "LexToken(ID,'abs',1,4)\n",
      "LexToken(NUMBER,965,1,8)\n"
     ]
    }
   ],
   "source": [
    "from ply.lex import lex, TOKEN\n",
    "\n",
    "class Lexer:\n",
    "    tokens = ( 'NUMBER', 'ID', 'WHITESPACE' )\n",
    "    \n",
    "    @TOKEN(r'\\d{1,5}')\n",
    "    def t_NUMBER(self, t):\n",
    "        t.value = int(t.value)\n",
    "        return t\n",
    "\n",
    "    @TOKEN(r'\\w+')\n",
    "    def t_ID(self, t):\n",
    "        return t\n",
    "\n",
    "    @TOKEN(r'\\s+')\n",
    "    def t_WHITESPACE(self, t):\n",
    "        pass\n",
    "\n",
    "    def t_error(self, t):\n",
    "        pass\n",
    "    \n",
    "\n",
    "__file__ = \"02.Tokenizers.ipynb\"    # make `ply` happy\n",
    "\n",
    "lexer = lex(object=Lexer())\n",
    "lexer.input('123 abs 965')\n",
    "for token in lexer:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
