1. Основные задачи в области обработки естественного языка. Понятие "языковой модели".
2. Классическая токенизация: регулярные выражения, достоинства, недостатки.
3. Морфологический анализ по правилам и стемминг. 
4. Контекстно-свободные грамматики. Иерархия Хомкского. Алгоритм Кока — Янгера — Касами и Эрли.
5. Использование Yargy, Томита-парсера  для извлечения структурированных данных из предложений на естественном языке.
6. CRF и HMM.
7. Применение модели "bag of words" (мешок слов) для решения некоторых задач обработки естественного языка. Кластеризация и классификация текстов. Понижение размерности. Достоинства и недостатки.
8. Тематическое моделирование (LSA, pLSA, LDA*).
9. Subword Tokenization. BPE и wordpiece. Достоинства и потенциальные проблемы.
10. Классические методы векторного представления слов. Word2vec, fastText. Решение некоторых задач в области обработки естественного языка.
11. PyTorch: основные концепции, процесс обучение, применение основных фреймворков. DDP*. 
12. Языковые модели на основе рекуррентных нейронных сетей: LSTM и GRU. Достоинства и недостатки по отношению к трансформерам. 
13. Трансформер. Общая архитектура и особенности (алгоритмическая сложность, память, распараллеливание, размер контекста). 
14. Решение базовых задач обработки естественного языка с помощью не очень больших предобученных языковых моделей (BERT, GTP2).
15. LLM. Проблемы обучения. Дообучение: supervised fine-tuning (SFT) и Reinforcement Learning from Human Feedback (RLHF).
16. RAG. 